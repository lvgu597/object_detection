### 目标检测

主要是两种类型算法：

+ **tow-stage**：先通过启发式方法或者CNN网路产生一系列稀疏的候选框，然后对这些后选款进行分类与回归
+ **one-stage**：均匀地在图片不同位置进行密集抽样，抽样时可以采用不同尺度和长度比，然后利用CNN提取特征后直接进行分类与回归

**名词解释**：

+ TP：分为正样本并且分对了 
+ TN：分为负样本并且分对了
+ FP：分为正样本，但是分错了
+ FN：分为负样本，但是分错了
+ Precision（精准率）: TP / (TP + FP)  
  + **分类器认为是正类并且确实是正类占所有预测为正类的比例**
+ Recall（召回率）: TP / (TP + FN)
  + **分类器认为是正类并且确实是正类占所有实际为正类的比例**
+ PR曲线：Precision-Recall曲线
+ AP：取不同置信度所获得不同PR值，将这些值绘制成的曲线的下面积即为该类的AP值
+ mean average precision(mAP)：各类别AP的平均值



+ RoI（region of interest）RoI是一个四元组$(r,c,h,w)$，$(r,c)$表示左上坐标，$h$为高，$w$为宽 一般指图像上的区域框



+  stochastic  gradient  descen（SGD）：梯度更新规则，常见的有BGD，SGD，MBGD等。SGD每次更新时对每个样本进行梯度更新，对于很大的数据集来说，可能会有相似的样本，这样BGD在计算梯度时会出现冗余，而SGD一次只进行一次更新，就没有冗余，而且比较快，并且可以新增样本

  更多优化器Optimizer算法[详见](https://www.cnblogs.com/guoyaohua/p/8542554.html)



+ $L_1$loss：绝对值损失 $L_1 = |f(x)-Y|$
+ $L_2$loss：平方损失 $L_2 = |f(x)-Y|^2$
+ Smooth L1 loss: $Smooth\space L_1 = 0.5x^2,|x|<1; |x|-0.5,x<-1\space or\space x>1$



+ NMS（non-maximum suppression）：非极大值抑制。对于同一个object，会检测出多个bounding box，可以通过NMS去除多余的bounding box，只保留ground truth重叠度高的bounding box，主要流程如下：
  + 首先对于每一类，将所有score<预设值(eg:0.3)的bbox的score设为0
  + 之后，按bbox score得分排序，选出其中得分最高的bbox
  + 遍历其余bbox，如果和最高bbox的IoU值大于一定阈值，则删除此bbox
  + 从未处理的bbox继续选择一个最高分的bbox，重复上述过程
  + 重复上述过程，直到找到全部保留的bbox
  + 根据所有保留bbox的score和class color画出最后预测结果





#### SSD(Single Shot MultiBox Detector)

+ 输入3\*300\*300 的image

+ 整个网络结构

  ```
  SSD(
    (vgg): ModuleList(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace=True)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))        
      (6): ReLU(inplace=True)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))       
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))      
      (11): ReLU(inplace=True)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))      
      (13): ReLU(inplace=True)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))      
      (15): ReLU(inplace=True)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))      
      (18): ReLU(inplace=True)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))      
      (20): ReLU(inplace=True)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace=True)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace=True)	
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace=True)
      (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
      (31): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))
      (32): ReLU(inplace=True)
      (33): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      (34): ReLU(inplace=True)
    )
    (conv8_1): Sequential(
      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): ReLU(inplace=True)
    )
    (conv8_2): Sequential(
      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): ReLU(inplace=True)
    )
    (conv9_1): Sequential(
      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
      (1): ReLU(inplace=True)
    )
    (conv9_2): Sequential(
      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): ReLU(inplace=True)
    )
    (conv10_1): Sequential(
      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
      (1): ReLU(inplace=True)
    )
    (conv10_2): Sequential(
      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU(inplace=True)
    )
    (conv11_1): Sequential(
      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
      (1): ReLU(inplace=True)
    )
    (conv11_2): Sequential(
      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))
      (1): ReLU(inplace=True)
    )
    (feature_map_loc_1): Sequential(
      (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (feature_map_loc_2): Sequential(
      (0): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (feature_map_loc_3): Sequential(
      (0): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (feature_map_loc_4): Sequential(
      (0): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (feature_map_loc_5): Sequential(
      (0): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (feature_map_loc_6): Sequential(
      (0): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (feature_map_conf_1): Sequential(
      (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (feature_map_conf_2): Sequential(
      (0): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (feature_map_conf_3): Sequential(
      (0): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (feature_map_conf_4): Sequential(
      (0): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (feature_map_conf_5): Sequential(
      (0): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (feature_map_conf_6): Sequential(
      (0): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  ```

+ 分别在conv4_3，conv7，conv8_2，conv9_2，conv10_2，conv11_2抽出feature map

+ 每个feature map包含多个指定长宽比的default box
+ 输出loc、conf值

**问题**：SSD最后计算的是loc以及conf的值，它是如何实现分类的？

+ 传统网络会在最后加fc层来实现分类，然后再利用滑动窗口的策略实现多目标的检测与分割，因此**全连接层的输出节点数即为分类数**

+ 全卷积神经网络直接使用卷积层代替全连接层，不再对整张图提取特征，而是提取多个滑动窗口的特征，因此**最后得出的是多个滑动窗口的更多个分类**，那么如何计算每个滑动窗口的类别？通过IOU

#### YOLO(You Only Look Once)

>  只运行一次cnn计算；
>
> 采用不同大小和比例（宽高比）的窗口在整张图片上以一定的步长进行滑动，然后对这些窗口对应的区域做图像分类

项目汇总：https://bbs.cvmart.net/topics/1511

#### 迁移学习

+ 将一个任务训练好的参数，拿到另一个任务作为其神经网络初始参数，从而提高精度的方法

### 疵点检测

https://github.com/zhengye1995/Tianchi-2019-Guangdong-Intelligent-identification-of-cloth-defects-rank5

https://github.com/cizhenshi/TianchiGuangdong2019_2th/?spm=5176.12282029.0.0.133830edNXUOwW

天池项目地址  https://tianchi.aliyun.com/competition/entrance/231748/forum

#### 打标签工具

labelme（工具）/ pip install lablelimg



#### 学习路径

+ 学习几个经典的detection网络
  + one stage: yolo系列、ssd系列
  + two stage：rcnn系列、fast-cnn
+ 了解一下这两年的anchor free的做法

+ 框架：mmdetection结合项目

结合mmdetection跑相关算法，然后相关源码，了解前沿技术

天池 Cascade-RCNN + resnet50

faster-Rcnn →知乎收藏文件夹两篇文章



**mmdetection**

建议使用 MMDetection。

首先，截至目前 MMDection 已经复现了深度学习时代目标检测相关的 57 篇经典论文，包括 SSD，RetinaNet, Faster R-CNN, Mask R-CNN, Cascade R-CNN, HTC 等等。题主可以按照论文发表的时间顺序将 MMDetection model zoo 中支持的算法以及论文都阅读一遍。在阅读论文的同时，可以同时尝试用 MMDetection 跑一跑该算法。

第二步，在通读系列论文以后，可以开始阅读一些经典论文的代码，例如 RetinaNet, Faster R-CNN, Mask R-CNN 等。可以重点关注  MMDetection 中 mmdet.core 中的 bbox assigner 和 sampler，以及这些方法在  mmdet.models.dense_heads, mmdet.models.roi_heads 中的对应模块。工业界目前使用的检测器大多都是基于这些经典方法的调优，但一些核心逻辑和概念是一致的，例如 NMS，IoU，FPN，Anchor 的生成，Anchor/Box 和 GT 框的匹配，RoI 的提取，FPN 等。

第三步，如果题主还想继续深入了解当前目标检测的现状，可以参考 MMDetection 的 project 页面[https://mmdetection.readthedocs.io/en/latest/projects.html](https://link.zhihu.com/?target=https%3A//mmdetection.readthedocs.io/en/latest/projects.html)，目前有很多学术界的前沿方法都是基于 MMDetection 来开发的，例如单阶段的实例分割算法 SOLO 和 PolarMask。题主同样可以体验边读论文，边跑算法的乐趣。

第四步，MMDetection 还提供了一个 colab tutorial [https://github.com/open-mmlab/mmdetection/blob/master/demo/MMDet_Tutorial.ipynb](https://link.zhihu.com/?target=https%3A//github.com/open-mmlab/mmdetection/blob/master/demo/MMDet_Tutorial.ipynb)。样例展示了使用 MMDetection model zoo 中的模型进行推理，并在新的数据集上进行微调的全过程。题主可以在尝试这个 colab tutorial 之后，尝试将模型应用于自己的数据集，并尝试调节一些模型的超参数来提高模型在数据集上的性能。



#### RCNN（Regions with CNN features）

黄色标记：结论	绿色标记：怎么做

【地址】https://arxiv.org/pdf/1311.2524.pdf

【主要贡献】1. CNN可用于目标检测 2. 数据紧缺时，先在一个小数据集上训练的模型，经过fine-tuning，然后再运用到大数据集上可以取得比较好的效果

【文章结构】介绍文章key insights→介绍模型设计→实验数据对比分析→数据集特点→结论

【算法思想】主要分为三个部分：生成区域proposals，CNN从每个区域提取特征，最后运用SVM分类。首先通过Selective Search方法筛选出一些备选的Region proposals；然后利用CNN进行特征提取，再将提取到的特征向量训练SVM，得到最终的分类结果；然后利用Non-Maximun Suppression对最终得到的bbox进行筛选；最后对bbox进行回归，修正bbox中的坐标的值，得到更精确的bbox。其中在训练阶段，运用了supervised pre-training以及确定区域的fine-tuning。

【tricks】1. IOU阈值的选择对结果影响比较大：一个用来识别正样本（如设置与ground truth的IOU值大于0.5），一个用来识别负样本（即背景类，如设置IOU<0.1），而基于两者之间的则为Hard Negative，若标记为正样本，则包含过多背景信息，标记为负样本，又包含待检测目标特征，这些proposal应该被忽略。

【算法缺陷】1. 训练分为三个阶段，比较复杂 2. 处理流程为图像→候选区域→CNN→特征→输出，每个区域都要经过一次CNN提取特征，时间和空间代价比较高 3.目标检测很慢，VGG16网络 47s / image (on a GPU) 

#### Fast R-CNN（Fast Region-based Convolutional Network）

【地址】https://arxiv.org/abs/1504.08083（**值得深入学习**）

【主要贡献】R-CNN对于每个region proposal都会经过CNN计算特征，Fast R-CNN通过对一整张图片计算一遍CNN得到feature map，然后利用RoI池化层对各个RoI分别计算特征从而提高效率；而且去掉SVM分类，改成softmax分类和bbox回归，利用一个合并的multi-task loss来训练，实现一定程度的优化

【文章结构】总分总的形式，详细介绍整个网络→网络每个部分→再次说明网络结构→实验结果对比分析→一些反思

【算法思想】对**整张图片**进行卷积、池化，再通过RoI池化层来从feature map中提取定长的特征向量，将这些得到的特征向量进行正负样本的整理，分batch传入并行的R-CNN子网络，每个特征向量通过全连接层分成分类和回归两个子输出层，一个产生k+1个分类，另一个得到bbox的位置，并将两者损失统一起来。

【tricks】1. RoI(Region of Interest)池化层：使用最大池化层将任意region of interest内的特征转化为具有固定空间范围（eg：$H*W$）的小的feature map（可我理解的是为按比例缩放的最大池化）；2. SVD能大大降低全连接层所消耗的时间而只带来很小的mAP损失 3.fine-tuned不应该被用在所有的卷基层 4. 多尺度相比单尺度精度略优提升，但时间开销更大，一定程度说明深度卷积网络可以内在地学习尺度不变性 5. 在更多的数据上训练，精度是有进一步提升的 6. softmax引入类间竞争，效果更好 7. 更多的proposals并不一定带来精度的提升

#### Faster R-CNN（Towards Real-Time Object Detection with Region Proposal Networks）

【地址】https://arxiv.org/abs/1506.01497

【主要贡献】利用共享卷积层的RPN来代替Selective Search算法，从而提高运行效率。RPN(Region Proposal Network): RPN是一种与**检测网络共享卷积层**的用于生成目标建议的全卷积网络（FCN）。引入anchor（图片的中心），RPN基于anchor box。实现端到端的检测，大大提高检测速度

【文章结构】基本介绍→相关工作→总分形式介绍算法模型→实验数据对比分析→总结

【算法思想】RPN+Fast R-CNN（RPN指出Fast R-CNN”看“的区域）。RPN以整张图片作为输入，输出一组object建议框，并且每个框都带有一个对象评分。anchor是一个滑动窗口的中心，每个W*H的feature map有W\*H\*k个anchor，anchor具有转移不变性以及能够解决多尺度预测。RPN网络的训练基于anchor box，按照不同的尺寸(scale)和比例(aspect ratio)预先定义k个anchor box，分类层生成2K维度的向量（object以及背景两个输出），回归层生成4K维度的向量（4个用来定位x,y,w,h）。训练步骤分为四步：1. 训练RPN 2. 利用第一步RPN生成的proposal训练一个Fast R-CNN的检测网络 3. 使用检测网络初始化RPN网络  4.保持共享的卷基层固定，微调Fast R-CNN特有的层 重复迭代此过程。共享卷积的三种思路：1. Alternating training 2. Approximate joint training 3. Non-approximate joint training

【tricks】1. RPN和object detection共享卷积，提高效率；2. 引入滑动窗口生成anchor box的思想，解决多尺度的问题


深入理解：https://blog.csdn.net/weixin_44791964/article/details/105739918?spm=1001.2014.3001.5501
1. 卷积层（13个卷积层，13个relu层，4个pooling层）

  > 所有conv层：kernel_size = 3,pad = 1,stride = 1 ##M\*N → M\*N (M-3+2)/1+1=M
  > 所有pooling层：kernel_size = 2,pad = 0, stride = 2 ##M*N → M/2 * N3/2 (M-2)/2+1=M/2
  > 最终一个M\*N大小矩阵经过conv layers固定变为(M/16)\*(N/16)
  > 得到feature maps共享特征层，features map实际为一个抽取的最后输出为batch_size*38\*38\*1024的卷积网络

2. RPN

    2.1 anchors

  > anchors可以理解为先验框；通过不同的scale以及比例形成k个(一般9个)anchors(一般有1:1,1:2,2:1三种)实际相当于引入多尺度的方法。
  > **每个feature map上的点上有k个anchor**，而每个anchor分为positive和negative，所以每个点由256d feature转为cls=2k scores；而每个anchor有(x,y,w,h) 四个偏移量，所以reg=4k coordinates。即38*38每个点转为18以及36两个维度；**anchors总数量为38\*38\*9=12996个，先验框shape为12996\*4**

  **RPN其实就是在原图尺度上，设置很多个先验Anchor，然后用CNN判断哪些Anchor是里面有目标的positive anchor，哪些是没有目标的negative anchor。softmax只做了一个二分类**

​	2.2 Proposal layer

  > 综合所有bbox regression偏移量和anchor计算出精准的proposal，即调整anchors生成相对精准的proposals即rois。利用score以及建议框调整值调整建议框，通过nms从12000多个先验框中筛选出比较准确的600个建议框，最后输出shape为600*4

3. RoI pooling

> 利用建议框rois在共享特征层feature map上进行截取并分区域池化从而得到同等shape的局部特征层
>
> feature map的shape为[batch_size,1024,38,38]、rois的shape为[600,4]、After Pooling的shape为[600,1024,14,14]

4. Resnet第五次压缩

   > 对每个建议框进行Resnet原有的第五次压缩。压缩完后进行一个平均池化，再进行一个Flatten，最后分别进行一个num_classes的全连接和(num_classes)x4全连接

   shape变化为：

   ![faster r-cnn.png](https://github.com/lvgu597/object_detection/blob/main/image/faster%20r-cnn.png)

5. 通过最后结果对建议框进行调整

   > 需要对建议框再做一次解码

**总共包含两次解码过程**：第一次获得的建议框解码后的结果是对共享特征层feature map进行截取；第二次是对最后建议框解码得到最终框。**整个网络是计算出每一个真实框所对应先验框的偏差**

### FPN: Feature Pyramid Networks for Object Detection(特征金字塔)

【地址】https://arxiv.org/abs/1612.03144

【主要贡献】提出一种简单、有效的建立特征金字塔的方式，在训练和测试时间基本不变的情况下大大提高识别率

【文章结构】分析已有解决方法利弊并提出取得很好效果的FPN→多尺度解决方法发展→FPN详细介绍→应用于实际网络诸如FPN、Faster-RCNN等→对比实验

【算法思想】传统解决多尺度问题的方法：

|                                                              | 缺陷                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1. 通过图像金字塔构建不同尺度的特征金字塔（通过不同尺度图像分别获取其对应特征） | 计算量大                                                     |
| 2. 利用单个高层特征进行预测                                  | 原始CNN采用的方法，对于小物体的特征无法保留                  |
| 3. 金字塔的特征层级（从不同卷积层抽取特征eg: SSD）           | 理想情况下会重用不同层的多尺度特在特征从而向传播过程中computer free，然而实际上为了避免使用低级特征，SSD在网络叫高层3开始抽取特征 |
| 4. FPN                                                       |                                                              |

以任意大小的单尺度图片作为输入， 以一种全卷积的方式输出多尺度的feature map，主要包括一下三个部分：自底向上的pathway，自顶向下pathway，以及一个lateral连接。可以简单将**底理解为高分辨率，顶理解为低分辨率**

> 理解 https://www.zhihu.com/search?type=content&q=fpn

**自底向上过程**和普通CNN没有区别，每个stage(输出size相同网络部分为同一个stage)对应一个特征金字塔的级别，以ResNet为例，选取conv2、conv3、conv4、conv5层的最后一个残差block层特征作为FPN的特征，记为{C2、C3、C4、C5}。这几个特征层相对于原图的步长分别为4、8、16、32。**自顶向下**通过上采样的方式将顶层(分辨率比较低的)图放大到上一个stage的特征图同样的大小，具体操作是C5首先经过1x1卷积得到M5特征，M5经过上采样的最近邻差值操作再加上C4经过1x1的卷积后的特征，得到M4，如此得到M3和M2。M层再经过3x3的卷积，最终得到P2、P3、P4、P5层，再加上P5直接降采样得到的P6，得到用于检测的五个特征图{32^2^,64^2^,128^2^,256^2^,512^2^}

【tricks】利用FPN解决多尺度问题

深入理解 https://zhuanlan.zhihu.com/p/35854548

1. 上采样

   采用线性插值进行上采样

2. 横向连接

   转换到相同维度之后直接相加；吐过不进行特征融合（即去掉所有1x1侧连接），虽然理论上分辨率没变，语义也增强了，但是AR下降了10%左右，作者认为这些特征上下采样太多次了，导致它们不适于定位。**Bottom-up的特征包含了更精确的位置信息。**

3. P2-P5最后又做一次3\*3的卷积

   消除上采样带来的混叠效应

4. 主要思想

   将高层特征传下来，补充底层的语义，这样可以获得高分辨率、强语义的特征，有利于小目标的检测。

5. 网络变化

   ![img](https://img-blog.csdnimg.cn/20200305145908257.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center)

#### Mask R-CNN

【地址】https://arxiv.org/abs/1703.06870

【主要贡献】1.RoI Align优化了RoI池化取整出现的musalignment问题 2. 通过对每个类别使用sigmod输出并计算二值交叉熵损失，从而避免类间竞争

【文章结构】Mask R-CNN及RoI Align简单介绍→相关工作→Mask R-CNN详细实现→其他细节参数设置→对比试验→拓展（人体）

【算法思想】相当于Faster R-CNN + mask(具体为ResNet-FPN + Fast RCNN + mask)。Mask RCNN就是在ROI Align之后添加卷积层，进行mask预测的任务。三个输出：类别标签、偏移bbox以及目标mask；

【tricks】1. ROI Align代替ROI pooling 利用双线性插值法 解决其在按比例池化需要取整时出现的候选框位置偏差问题(misalignment) 2. 增加mask分支

深入理解：就是在原本最后两个分支（分类和回归坐标）增加一个分支进行语义分割

1. mask语义分割信息的获取

利用类似faster R-CNN中得到的最终预测框，**作为mask模型的区域截取部分**，利用预测框对mask模型中用到共用特征层进行截取。截取后，利用mask模型再对像素点进行分类，获得语义分割结果

#### Cascade R-CNN

【地址】https://arxiv.org/abs/1712.00726

【主要贡献】

【文章结构】

【算法思想】

【tricks】

理解：https://blog.csdn.net/u014380165/article/details/80602027

#### TridentNet

【地址】

【主要贡献】处理目标检测中尺度变换的新思路

【文章结构】

【算法思想】

【tricks】

理解：https://zhuanlan.zhihu.com/p/54334986

#### SSD: Single Shot MultiBox Detector 

【地址】https://arxiv.org/abs/1512.02325

【主要贡献】

【文章结构】

【算法思想】

【tricks】

#### Deep Learning for Generic Object Detection: A Survey（综述2019）

【地址】https://arxiv.org/abs/1809.02165



DCN2

yolo系列

transformers 数据增强操作？

GCN

再到具体方向



#### Q：

目标检测的多尺度（multi-scale training）问题是指？

> 输入图片的尺寸对检测模型的性能影响相当明显，多尺度是提升精度最明显的技巧之一。
>
> 在基础网络部分常常会生成比原图小数十倍的特征图（feature map），导致小物体的特征描述不容易被检测网络捕捉。通过输入更大、更多尺寸的图片进行训练，能够在一定程度上提高检测模型对物体大小的鲁棒性，仅在测试阶段引入多尺度，也可享受大尺寸和多尺寸带来的增益。

为什么要设置正负样本？（一般设置IOU>0.7为正样本，IOU<0.3位负样本？）

> 正样本是任务所要检测的目标物；负样本是则是目标物所处的不同背景

特征金字塔FPN（Feature Pyramid Networks）？

> FPN是一种高效的特征提取方法，能解决多尺度问题
>
> FCN也就是Fully Convolutional Network，是一个不包含全连接层的网络。FCN本质等效为密集滑窗，因此不需要显示地移动滑动窗口以处理不同位置的目标。

IOU和ROI的区别

> IoU = $\frac{DetectionResult \space and \space GroundTruth}{DetectionResult \space or \space GroundTruth}$
>
> RoI(Region of Interest)  感兴趣区域，一般指图像上的区域框

规范化BN(Batch Normalization)和GN(Group Normalization)？

> https://zhuanlan.zhihu.com/p/115949091
>
> 批规范化(BN)：在minibatch维度上在每次训练iteration时对隐藏层进行归一化
>
> 标准化：对输入数据进行归一化处理
>
> 正则化：通常指对参数在量级和尺度上做约束，缓和过拟合情况，eg：L1、L2正则化
>
> GN(Group Normalization)：GN先对通道进行分组，每个组内的所有$C_i,W,H$维度求均值方差进行规范化。与batch size无关

val数据集有什么用？

> val是validation简称
>
> validation set 对最终训练模型没有贡献，是用来检查训练结果是否过拟合的。

为什么模型层数增加到某种程度，模型效果不升反降？

> https://zhuanlan.zhihu.com/p/77794592
>
> 特征随着层层前向传播得到完整保留的可能性微乎其微

目标检测，语义分割(semantic segmentation)、实例分割(instance segmentation)区别？

> instance segmentation requires the correct detection of all objects in an image while also precisely segmenting each instance. it combines object detection and localize each using a bounding box. 
>
> 语义分割在分割时对同一物体一视同仁，而实例分割会将每个个体单独分割出来

1\*1的卷积与3\*3的卷积有什么用？

> 1\*1的卷积用来压缩通道数；3\*3的卷积输出结果不变，用来提取特征

#### coco数据格式

```python
#---------------------------------------------------------------------------------------
# coco文件结构为:
# -csv/
# ---labels.csv
# ---images/
# ------images1.jpg
# ------images2.jpg
# labesl.csv 形式为: /path/to/image,xmin,ymin,xmax,ymax,label
# eg: /mfs/dataset/face/0d4c5e4f-fc3c-4d5a-906c-105.jpg,450,154,754,341,face
#     /mfs/dataset/face/0ddfc5aea-fcdac-421-92dad-144.jpg,143,154,344,341,face
#     ...
#--------------------------------------------------------------------------------------

<class 'dict'>
{
    "info": info, #dict
    "licence": [license], #list. 内部为list
    "images":[
        {
            "id":, #图片唯一id 可以用自增的img_id
            "filename":,
            "height":,
            "width":,
        }
    ],
    "annotations":[
        {
            "segmentation": [ #对象的边界点（边界为多边形）
                [
                    x1,y1, #第一个点x,y坐标
                    x2,y2, #第二个点x,y坐标
                    x3,y3,
                ]
            ],
            "area": 1481.12312312,#区域面积
            "iscrowd":0,
            "image_id": ,#对应的图片id（与images中的id对应） 可以用自增的img_id
            "bbox": [x,y,w,h],
            "category_id":, #类别id(与categories中的id对应)
            "id": ,#对象id，每个图片有不止一个对象，所以对每个对象编码（每个对象id唯一） 可以用自增的ann_id
        }
    ],
    "categories":[
        {
            "suppercategory": "person", #主类别
            "id":1, #类对饮的id（0默认为背景）
            "name":"person" #子类别
        }
    ]
}
```

#### VOC数据格式

````python
# -------------------------------------------------------
# voc 文件结构:
# -VOC2007/
# ---Annotations/
# ------0d4c5e4f-fc3c-4d5a-906c-105.xml
# ------0ddfc5aea-fcdac-421-92dad-144.xml
# ------...
# ---ImageSets/
# ------Main/
# ----------train.txt
# ----------test.txt
# ----------val.txt
# ----------trainval.txt #所有训练和验证数据
# ---JPEGImages/
# ------0d4c5e4f-fc3c-4d5a-906c-105.jpg
# ------0ddfc5aea-fcdac-421-92dad-144.jpg
# ------...
#--------------------------------------------------------
<annotation>
  <folder>17</folder> # 图片所处文件夹
  <filename>77258.bmp</filename> # 图片名
  <path>~/frcnn-image/61/ADAS/image/frcnn-image/17/77258.bmp</path>
  <source>  #图片来源相关信息
    <database>Unknown</database>  
  </source>
  <size> #图片尺寸
    <width>640</width>
    <height>480</height>
    <depth>3</depth>
  </size>
  <segmented>0</segmented>  #是否有分割label
  <object> 包含的物体
    <name>car</name>  #物体类别
    <pose>Unspecified</pose>  #物体的姿态
    <truncated>0</truncated>  #物体是否被部分遮挡（>15%）
    <difficult>0</difficult>  #是否为难以辨识的物体， 主要指要结体背景才能判断出类别的物体。虽有标注，但一般忽略这类物体
    <bndbox>  #物体的bound box
      <xmin>2</xmin>
      <ymin>156</ymin>
      <xmax>111</xmax>
      <ymax>259</ymax>
    </bndbox>
  </object>
</annotation>
````

